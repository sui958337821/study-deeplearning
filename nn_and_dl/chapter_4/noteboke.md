## 训练数据和测试数据
机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和 实验等。首先，使用训练数据进行学习，寻找最优的参数;然后，使用测试 数据评价训练得到的模型的实际能力。为什么需要将数据分为训练数据和测 试数据呢?因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能 力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。
泛化能力是指处理未被观察过的数据(不包含在训练数据中的数据)的 能力。获得泛化能力是机器学习的最终目标。比如，在识别手写数字的问题 中，泛化能力可能会被用在自动读取明信片的邮政编码的系统上。此时，手 写数字识别就必须具备较高的识别“某个人”写的字的能力。注意这里不是“特 定的某个人写的特定的文字”，而是“任意一个人写的任意文字”。如果系统 只能正确识别已有的训练数据，那有可能是只学习到了训练数据中的个人的 习惯写法。
因此，仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。 这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。顺 便说一下，只对某个数据集过度拟合的状态称为过拟合(over fitting)。避免 过拟合也是机器学习的一个重要课题。
## 损失函数
神经网络以某个指标为线索寻找最优权重参数。神经网络的学习中 所用的指标称为损失函数(loss function)。这个损失函数可以使用任意函数， 但一般用均方误差和交叉熵误差等。

## 均方差函数
## 交叉熵误差

## 为什么要设定损失函数
上面我们讨论了损失函数，可能有人要问:“为什么要导入损失函数呢?” 以数字识别任务为例，我们想获得的是能提高识别精度的参数，特意再导入 一个损失函数不是有些重复劳动吗?也就是说，既然我们的目标是获得使识 别精度尽可能高的神经网络，那不是应该把识别精度作为指标吗?
对于这一疑问，我们可以根据“导数”在神经网络学习中的作用来回答。 下一节中会详细说到，在神经网络的学习中，寻找最优参数(权重和偏置)时， 要寻找使损失函数的值尽可能小的参数。为了找到使损失函数的值尽可能小 的地方，需要计算参数的导数(确切地讲是梯度)，然后以这个导数为指引， 逐步更新参数的值。
假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参 数。此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权 重参数的值，损失函数的值会如何变化”。如果导数的值为负，通过使该权 重参数向正方向改变，可以减小损失函数的值;反过来，如果导数的值为正， 则通过使该权重参数向负方向改变，可以减小损失函数的值。不过，当导数 的值为 0 时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此 时该权重参数的更新会停在此处。
之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数 都会变为 0，导致参数无法更新。话说得有点多了，我们来总结一下上面的内容。
在进行神经网络的学习时，不能将识别精度作为指标。因为如果以 识别精度为指标，则参数的导数在绝大多数地方都会变为 0。
为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成 0
                        
呢?为了回答这个问题，我们来思考另一个具体例子。假设某个神经网络正 确识别出了 100 笔训练数据中的 32 笔，此时识别精度为 32 %。如果以识别精 度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32 %，不会 出现变化。也就是说，仅仅微调参数，是无法改善识别精度的。即便识别精 度有所改善，它的值也不会像 32.0123 . . . % 这样连续变化，而是变为 33 %、 34 % 这样的不连续的、离散的值。而如果把损失函数作为指标，则当前损 失函数的值可以表示为 0.92543 . . . 这样的值。并且，如果稍微改变一下参数 的值，对应的损失函数也会像 0.93432 . . . 这样发生连续性的变化。
识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值 也是不连续地、突然地变化。作为激活函数的阶跃函数也有同样的情况。出 于相同的原因，如果使用阶跃函数作为激活函数，神经网络的学习将无法进行。 如图4-4所示，阶跃函数的导数在绝大多数地方(除了0以外的地方)均为0。 也就是说，如果使用了阶跃函数，那么即便将损失函数作为指标，参数的微 小变化也会被阶跃函数抹杀，导致损失函数的值不会产生任何变化。
阶跃函数就像“竹筒敲石”一样，只在某个瞬间产生变化。而 sigmoid 函数， 如图 4-4 所示，不仅函数的输出(竖轴的值)是连续变化的，曲线的斜率(导数) 也是连续变化的。也就是说，sigmoid 函数的导数在任何地方都不为 0。这对 神经网络的学习非常重要。得益于这个斜率不会为 0 的性质，神经网络的学 习得以正确进行。